{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Ensure NLTK resources are downloaded once\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load dataset\n",
    "DATASET_PATH = os.getenv(\"DATASET_PATH\", \"../data/dev.json\")\n",
    "df = pd.read_json(DATASET_PATH)\n",
    "\n"
   ],
   "id": "ba956aa2ea53dd50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Schema extraction from CSV\n",
    "def get_table_schema_from_csv(csv_path: str) -> dict:\n",
    "    df_csv = pd.read_csv(csv_path, nrows=0)\n",
    "    table_name = os.path.basename(csv_path).replace(\".csv\", \"\")\n",
    "    columns = df_csv[\"column_name\"]\n",
    "    column_description = df_csv[\"column_description\"] if \"column_description\" in df_csv.columns else [\"\"]\n",
    "\n",
    "    return {\n",
    "        \"table_name\": table_name,\n",
    "        \"columns\": columns,\n",
    "        \"table_description\": column_description\n",
    "    }\n",
    "\n",
    "\n",
    "# Tokenization and lemmatization\n",
    "def tokenize_and_lemmatize(question):\n",
    "    tokens = word_tokenize(question)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    irrelevant_pos = {\"DT\", \".\", \",\", \":\", \"''\", \"``\", \"(\", \")\"}\n",
    "\n",
    "    return [\n",
    "        [token, lemmatizer.lemmatize(token.lower()), pos]\n",
    "        for token, pos in pos_tags if pos not in irrelevant_pos\n",
    "    ]\n",
    "\n",
    "\n",
    "# Apply tokenization and lemmatization\n",
    "df[\"tokens\"] = df[\"question\"].apply(tokenize_and_lemmatize)\n",
    "\n",
    "# Prepare lemmas for TF-IDF\n",
    "df[\"lemmas_str\"] = df[\"tokens\"].apply(lambda toks: \" \".join(lemma for _, lemma, _ in toks))\n",
    "df"
   ],
   "id": "dc52a67b3866c2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Compute TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"lemmas_str\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# Attach TF-IDF scores to tokens\n",
    "def attach_tfidf(tokens, doc_index):\n",
    "    tfidf_scores = tfidf_matrix[doc_index].toarray().flatten()\n",
    "    scores_dict = dict(zip(feature_names, tfidf_scores))\n",
    "\n",
    "    return [\n",
    "        [token, lemma, pos, scores_dict.get(lemma, 0.0)]\n",
    "        for token, lemma, pos in tokens\n",
    "    ]\n",
    "\n",
    "\n",
    "df[\"tokens\"] = df.apply(lambda row: attach_tfidf(row[\"tokens\"], row.name), axis=1)\n",
    "\n",
    "# Filter tokens based on TF-IDF threshold (70% of max)\n",
    "df[\"filtered_tokens\"] = df[\"tokens\"].apply(\n",
    "    lambda tokens: [\n",
    "        [token, lemma, pos, float(score)]\n",
    "        for token, lemma, pos, score in tokens\n",
    "        if score >= 0.6 * max(s[3] for s in tokens)\n",
    "    ]\n",
    ")\n",
    "df\n"
   ],
   "id": "df3ee4020cea2099",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from llm.llm_request import call_llm_model\n",
    "\n",
    "\n",
    "# Format prompt for LLM\n",
    "def format_prompt(tokens: list, table_info: dict, question: str):\n",
    "    token_texts = [token[0] for token in tokens if token[0]]\n",
    "    token_texts = list(set(token_texts))\n",
    "\n",
    "    tables_and_columns = []\n",
    "    for table in table_info['table_name']:\n",
    "        cols = table_info['columns'][table]\n",
    "        column_entries = [\n",
    "            f\"- {col['column_name']}: {col['column_description']}\" if col[\n",
    "                'column_description'] else f\"- {col['column_name']}\"\n",
    "            for col in cols\n",
    "        ]\n",
    "        tables_and_columns.append(f\"TABLE: {table}\\nCOLUMNS:\\n\" + \"\\n\".join(column_entries))\n",
    "\n",
    "    tables_and_columns_str = \"\\n\\n\".join(tables_and_columns)\n",
    "\n",
    "    return f\"\"\"Given the following tables and columns information:\n",
    "{tables_and_columns_str}\n",
    "\n",
    "Relevant keywords extracted from the user's question: {', '.join(token_texts)}\n",
    "\n",
    "User question: {question}\n",
    "\n",
    "Map each relevant keyword to the most appropriate column and corresponding table from the information above. Provide your answer strictly as a JSON object. Do not include any explanations or text outside the JSON structure. Only include mappings you are confident about.\n",
    "\n",
    "Example output:\n",
    "{{\n",
    "    \"year\": {{\n",
    "        \"column_name\": \"year\",\n",
    "        \"table_name\": \"movies\"\n",
    "    }},\n",
    "    \"sales_amount\": {{\n",
    "        \"column_name\": \"amount\",\n",
    "        \"table_name\": \"sales\"\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Extract only JSON object from LLM output\n",
    "def extract_json_from_response(response_text):\n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    stack = []\n",
    "    start = None\n",
    "\n",
    "    for i, char in enumerate(response_text):\n",
    "        if char == '{':\n",
    "            if not stack:\n",
    "                start = i\n",
    "            stack.append(char)\n",
    "        elif char == '}':\n",
    "            if stack:\n",
    "                stack.pop()\n",
    "                if not stack and start is not None:\n",
    "                    json_candidate = response_text[start:i + 1]\n",
    "                    try:\n",
    "                        return json.loads(json_candidate)\n",
    "                    except json.JSONDecodeError:\n",
    "                        logging.warning(\"Found braces but content is not valid JSON.\")\n",
    "                        return {\n",
    "                            \"mapping\": f\"{response_text}\"\n",
    "                        }\n",
    "\n",
    "    logging.warning(\"No valid JSON object found in the response.\")\n",
    "    return {\n",
    "        \"mapping\": f\"{response_text}\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Main function to get mappings using LLM\n",
    "def get_token_column_mapping(row) -> Any:\n",
    "    tables_path = os.path.join(\"../data/dev_databases\", row['db_id'], \"database_description\")\n",
    "    table_names = []\n",
    "    columns_info = {}\n",
    "\n",
    "    for file in os.listdir(tables_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df_file = pd.read_csv(os.path.join(tables_path, file), encoding_errors='replace')\n",
    "            table_name = file.replace(\".csv\", \"\")\n",
    "            table_names.append(table_name)\n",
    "\n",
    "            if \"original_column_name\" in df_file.columns:\n",
    "                descriptions = df_file[\"original_column_name\"].fillna(\"\").astype(str).tolist()\n",
    "            else:\n",
    "                descriptions = [\"\"] * len(df_file)\n",
    "\n",
    "            columns = [\n",
    "                {\n",
    "                    'column_name': str(col_name),\n",
    "                    'column_description': desc\n",
    "                }\n",
    "                for col_name, desc in zip(df_file[\"original_column_name\"], descriptions)\n",
    "            ]\n",
    "\n",
    "            columns_info[table_name] = columns\n",
    "\n",
    "    flattened_table_info = {\n",
    "        'table_name': table_names,\n",
    "        'columns': columns_info\n",
    "    }\n",
    "\n",
    "    prompt = format_prompt(row[\"filtered_tokens\"], flattened_table_info, row[\"question\"])\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "    try:\n",
    "        res = call_llm_model({\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 512,\n",
    "            \"top_k\": 2,\n",
    "            \"top_p\": 0.9,\n",
    "        }, model_id=model_id)\n",
    "\n",
    "        parsed = extract_json_from_response(res)\n",
    "        logging.info(f\"Processed {row['question_id']} ({row.name + 1}/{len(df)})\")\n",
    "        return json.dumps(parsed)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process row {row.name}: {e}\")\n",
    "        return json.dumps({})\n",
    "\n",
    "\n",
    "# Apply the function to get mappings\n",
    "df[\"token_column_mapping\"] = df.apply(get_token_column_mapping, axis=1)\n"
   ],
   "id": "a7d0f6d1219d8940",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the enriched dataset\n",
    "OUTPUT_PATH = os.getenv(\"OUTPUT_PATH\", \"../data/dev_enriched.json\")\n",
    "\n",
    "json_data = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    json_data.append({\n",
    "        \"question_id\": row['question_id'],\n",
    "        \"db_id\": row['db_id'],\n",
    "        \"question\": row['question'],\n",
    "        \"evidence\": row['evidence'],\n",
    "        \"SQL\": row['SQL'],\n",
    "        \"difficulty\": row['difficulty'],\n",
    "        \"filtered_tokens\": row['filtered_tokens'],\n",
    "        \"token_column_mapping\": json.loads(row['token_column_mapping'])\n",
    "    })\n",
    "\n",
    "with open(OUTPUT_PATH, 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)"
   ],
   "id": "7c512a8e568b4d78",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
